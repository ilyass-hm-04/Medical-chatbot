{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "06ee42ae",
            "metadata": {},
            "source": [
                "# Medical Chatbot Research Notebook\n",
                "\n",
                "This notebook demonstrates the RAG (Retrieval-Augmented Generation) pipeline for the Medical Chatbot. \n",
                "It covers data loading, text splitting, embedding generation, vector store creation (Pinecone), and the QA chain setup."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd374347",
            "metadata": {},
            "source": [
                "## 1. Environment Setup & Imports\n",
                "\n",
                "Load environment variables suitable for the project and import necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c77bac4d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from langchain.embeddings import HuggingFaceEmbeddings\n",
                "from pinecone import Pinecone, ServerlessSpec\n",
                "from langchain_pinecone import PineconeVectorStore\n",
                "from langchain.chat_models import ChatOpenAI\n",
                "from langchain.schema import HumanMessage\n",
                "from langchain.chains import create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
                "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
                "\n",
                "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
                "os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c8b6bf8e",
            "metadata": {},
            "source": [
                "## 2. data Loading\n",
                "\n",
                "We load PDF documents from the `data` directory. Note: Ensure you are in the correct directory or adjust the path."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0edbb345",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure we are pointing to the correct data directory.\n",
                "# If running from 'research' folder, data might be in '../data'\n",
                "# os.chdir(\"../\") # Uncomment if needed to change to root directory\n",
                "\n",
                "def load_pdf_file(data_path):\n",
                "    loader = DirectoryLoader(\n",
                "        data_path,\n",
                "        glob=\"*.pdf\",\n",
                "        loader_cls=PyPDFLoader\n",
                "    )\n",
                "    documents = loader.load()\n",
                "    return documents\n",
                "\n",
                "# Assuming 'data' folder is in the current or parent directory\n",
                "extracted_documents = load_pdf_file(\"data\")\n",
                "print(f\"Loaded {len(extracted_documents)} pages/documents.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f77f1018",
            "metadata": {},
            "source": [
                "## 3. Text Splitting\n",
                "\n",
                "Split the loaded documents into smaller chunks for efficient embedding and retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2525618",
            "metadata": {},
            "outputs": [],
            "source": [
                "def text_split(docs):\n",
                "    text_splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=500,\n",
                "        chunk_overlap=20,\n",
                "    )\n",
                "    texts_chunk = text_splitter.split_documents(docs)\n",
                "    return texts_chunk\n",
                "\n",
                "texts_chunk = text_split(extracted_documents)\n",
                "print(f\"Created {len(texts_chunk)} chunks.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3cbf6206",
            "metadata": {},
            "source": [
                "## 4. Embeddings\n",
                "\n",
                "Initialize HuggingFace embeddings (`all-MiniLM-L6-v2`) to convert text into vector representations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a28ce2cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_embeddings():\n",
                "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
                "    return embeddings\n",
                "\n",
                "embedding = download_embeddings()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e7ad1a48",
            "metadata": {},
            "source": [
                "## 5. Vector Store (Pinecone)\n",
                "\n",
                "Set up the Pinecone index and store the document embeddings. \n",
                "**Note:** Code to create a new index is included but commented out if it already exists."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "478feb8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "index_name = \"medical-chatbot\"\n",
                "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
                "\n",
                "# Check if index exists, create if not (Optional)\n",
                "if index_name not in pc.list_indexes().names():\n",
                "    pc.create_index(\n",
                "        name=index_name,\n",
                "        dimension=384,\n",
                "        metric=\"cosine\",\n",
                "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
                "    )\n",
                "\n",
                "# Upload NEW documents:\n",
                "docsearch = PineconeVectorStore.from_documents(\n",
                "    documents=texts_chunk,\n",
                "    embedding=embedding,\n",
                "    index_name=index_name\n",
                ")\n",
                "\n",
                "# Connect to the existing index\n",
                "# docsearch = PineconeVectorStore.from_existing_index(\n",
                "#     index_name=index_name,\n",
                "#     embedding=embedding,\n",
                "# )\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5a26250",
            "metadata": {},
            "source": [
                "## 6. Retrieval & LLM Setup\n",
                "\n",
                "Set up the retriever and the Chat Model (using OpenRouter/OpenAI compatible interface)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "878cdc47",
            "metadata": {},
            "outputs": [],
            "source": [
                "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
                "\n",
                "# Test Retriever\n",
                "retrieved_doc = retriever.invoke(\"What is Acne?\")\n",
                "# print(retrieved_doc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ffb935c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "chat = ChatOpenAI(\n",
                "    model_name=\"openai/gpt-4o-mini\", \n",
                "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
                "    api_key=OPENROUTER_API_KEY\n",
                ")\n",
                "\n",
                "# Simple Test\n",
                "msg = [HumanMessage(content=\"Say hi\")]\n",
                "resp = chat(msg)\n",
                "print(resp.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad5369eb",
            "metadata": {},
            "source": [
                "## 7. RAG Chain\n",
                "\n",
                "Combine the retriever and the LLM into a Question-Answering chain."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3fc9b7ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "system_prompt = (\n",
                "    \"You are an Medical assistant for question-answering tasks. \"\n",
                "    \"Use the following pieces of retrieved context to answer \"\n",
                "    \"the question. If you don't know the answer, say that you \"\n",
                "    \"don't know. Use three sentences maximum and keep the \"\n",
                "    \"answer concise.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", system_prompt),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "question_answer_chain = create_stuff_documents_chain(chat, prompt)\n",
                "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d5b2741",
            "metadata": {},
            "source": [
                "## 8. Testing the Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7247efe0",
            "metadata": {},
            "outputs": [],
            "source": [
                "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
                "print(\"Answer:\", response[\"answer\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aaaa141e",
            "metadata": {},
            "outputs": [],
            "source": [
                "response = rag_chain.invoke({\"input\": \"What is Acne?\"})\n",
                "print(\"Answer:\", response[\"answer\"])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
